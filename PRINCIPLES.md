# Principles

Logo made programming a medium for thinking about thinking. We want to make working with agents a medium for thinking about communication, coordination, and collaboration with other minds.

The turtle worked because it connected formal geometry to spatial reasoning children already had—navigation they'd done since infancy. We're not teaching something new. We're connecting formal understanding of context, attention, and instruction to *social reasoning people already have*.

You already know what it's like to be misunderstood. To hold too many things in mind. To speak differently to different people. To confidently say something wrong. These are the body-syntonic anchors for Agentstorms.

---

## 1. Coordination, Not Control

Working with an agent is like coordinating with another mind—because that's what it is. You already know this feeling: adjusting how you explain something when it's not landing, intuiting what context someone is missing, noticing when words mean different things to different people.

Agentstorms externalizes this coordination. The interface makes visible what you usually navigate by intuition: what context is shared, what instructions are shaping interpretation, where meaning diverged. The goal isn't to control the agent—it's to get good at the collaboration.

## 2. Miscommunication Is the Curriculum

When the agent does something unexpected, that's not a bug—it's the entire learning opportunity. You already know the felt sense of miscommunication: "That's not what I meant." "You're missing context." "We're talking past each other."

Agent Sandbox makes this explicit. Here's what you said. Here's what landed. Where did it diverge? Debugging becomes relationship repair—a pattern you already know. The environment echoes the rhythm of clarifying, restating, and realigning that characterizes all successful coordination.

## 3. Feel the Constraints From Inside

Before showing what the agent did, consider: what would *you* say, given only this context and these instructions? The learner should feel the constraints from inside, then see how the agent responded. Divergence becomes viscerally interesting—"Huh, I read that differently."

This inverts the typical flow. Instead of observing agent behavior and trying to explain it, you predict first, then compare. The experience of "same input, different interpretation" isn't abstract—it's the surprise you feel when someone reads your email completely differently than you intended.

## 4. Low Floor, Wide Walls, High Ceiling

Entry is familiar: just talk to someone. The interaction feels like communication because it is communication. You don't need to learn a new interface; you need to notice what you're already doing.

But the environment reveals layers as you're ready. Start by chatting and noticing patterns. Progress to seeing what context shapes responses. Eventually, compose reusable instruction sets and context kits. Wide walls mean many kinds of work: writing, analysis, code, research, creative projects—not one kind of task.

## 5. You Build Things That Matter

Papert's core claim: learning happens through making things you care about. The artifact is yours; the learning is incidental.

What do you make in Agent Sandbox? Not "experiments with a system" but artifacts: an instruction set that actually works for your recurring task, a context kit you reuse, a communication pattern you've refined. These are things you'd make anyway in the course of real work. The sandbox just makes the craft visible.

## 6. The Work Is Real Work

The tasks people do in Agent Sandbox should be tasks they actually want done—real emails, real code, real analysis. Not toy problems designed for learning.

Intuition develops through repeated engagement with real stakes. Artificial exercises teach artificial patterns. The learning happens because you're trying to get something done, and the environment makes your coordination visible while you do it.

## 7. Context as Peripheral Vision

You already know what it's like to have focal attention and peripheral awareness. Information slips from focus to periphery to gone. You've felt the limits of working memory.

We make context feel like that. Items in full context are sharp; items at the edge degrade, blur. The token limit isn't an abstract number—it's the felt experience of "I can't hold all of this." Adding more context has friction. The interface performs cognitive load rather than just measuring it.

## 8. The Learner Drives

Autonomous agent loops optimize for task completion. Learning loops optimize for understanding. The learner decides when to proceed, when to retry, when to branch. The pause between steps is where noticing happens.

But this isn't about control—it's about pacing the coordination. In a real conversation, you can slow down, ask clarifying questions, restate. Agent Sandbox gives you the same affordance with an agent: the ability to pause, examine, adjust, and re-engage.

## 9. Visible Thinking

The agent's reasoning stream isn't a debug panel you toggle on—it's the primary interface. You watch the agent interpret, consider, act. This mirrors what you're always trying to do in coordination: understand what the other party is thinking.

The difference is that the agent can actually show you. In human communication, you infer intent from behavior. Here, intent is legible. This is the superpower of the environment: you can see inside the coordination in a way you never can with humans.

## 10. Snapshots and Branches

At any point, you can snapshot the full state and branch. "What if I'd said it differently?" becomes testable. This is something you wish you could do in real communication—rewind and try again, see what would have happened.

The branching transforms single-path prompting into an explorable tree. You develop intuition for how small changes cascade, which you then carry into contexts where you can't branch.

## 11. Powerful Ideas Transfer

Papert cared about ideas that, once grasped, reorganize thinking across domains. Recursion learned through Logo helps you see recursion everywhere.

Working with agents should teach ideas that apply beyond agents:

- **Shared context isn't automatic**: What you include changes what something means. Applies to: writing, teaching, management, any communication where you've assumed shared knowledge that isn't shared.
- **Clarity is receiver-relative**: Instructions that seem clear to you may be ambiguous to another mind. Applies to: delegation, documentation, cross-cultural communication.
- **Repair is part of the process**: Misalignment isn't failure; it's information. The iteration *is* the work. Applies to: relationships, collaboration, any coordination.
- **Attention is finite and must be curated**: You can't include everything; prioritization is a skill. Applies to: focus, information architecture, deciding what matters.

These are communication insights, not AI insights. Agent Sandbox makes them concrete and practicable.

## 12. Everyone Already Knows How to Do This

The accessibility claim isn't "this is simple" or "we've removed the complexity." It's "you already have the underlying skill."

You've been coordinating with other minds your whole life. You've repaired miscommunications, adjusted your framing for different audiences, noticed when someone wasn't following. Agent Sandbox connects formal understanding of context and attention to this existing competence.

The risk is replicating the exclusion Papert identified: people comfortable with abstraction learn prompt engineering, people who need embodied grounding get left behind. The body-syntonic anchor—social cognition—is the counter. Everyone has it.

---

## Open Tensions

**What might be irreducibly different:**
The turtle had continuous feedback—watch the line emerge, stop mid-shape, see where you are. Fine grain. Agent reasoning is chunkier: input, processing, output. The sense of "I am moving through this step by step" is weaker.

Options: (1) Make the grain finer with step-by-step execution and mid-thought intervention. But this might build a fiction of continuous control that doesn't match the technology. (2) Accept the difference. Logo taught spatial-procedural thinking because the turtle was spatial-procedural. Agents might teach interpretive-collaborative reasoning. Different substrate, different skill.

**Tool vs. entity:**
Is the agent a tool you use or an entity you coordinate with? We've said "coordinate," but the design will constantly force choices. When you're debugging, do you think "I need to fix my input" (tool framing) or "we need to get on the same page" (entity framing)? Both are true. The interface should support both without forcing a choice.

**Real work vs. safe experimentation:**
Real work has real stakes; safe experimentation requires consequence-free branching. A "dry-run" mode might help—work on real tasks with real context, but defer consequences. The agent shows what it *would* do; you decide when it actually happens. How do we make dry-run feel different from live? Not just a toggle—something you feel. Visual weight, pacing, a deliberate "commit" gesture.

**Measuring transfer:**
We claim ideas transfer beyond agents. How would we know? Self-report is weak. Behavioral change is hard to track. Maybe: do people who use Agent Sandbox communicate differently in other contexts? Do they structure documents differently, delegate differently, explain differently? Worth designing for eventual measurement.
